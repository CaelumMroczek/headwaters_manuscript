g
training_data <- read_csv(here("1-data/WUS-D3_climate_data/training_data.csv"))
library(tidyverse)
training_data <- read_csv(here("1-data/WUS-D3_climate_data/training_data.csv"))
library(dplyr)
library(here)
training_data <- read_csv(here("1-data/WUS-D3_climate_data/training_data.csv"))
View(training_data)
eval_ssp245 <- read_csv(here("1-data/WUS-D3_climate_data/eval_data_ssp245.csv"))
library(tidyverse)
library(tidyverse)
library(here)
eval_ssp245 <- read_csv(here("1-data/WUS-D3_climate_data/eval_data_ssp245.csv"))
eval_ssp585 <- read_csv(here("1-data/WUS-D3_climate_data/eval_data_ssp585.csv"))
training_data <- read_csv(here("1-data/training_data.csv"))
enrich_eval_data <- function(df, clusters_df = NULL) {
enriched <- df |>
arrange(basin_id, year, month) |>
group_by(basin_id) |>
mutate(
prec_lag1 = lag(prec, 1),
prec_lag2 = lag(prec, 2),
prec_lag3 = lag(prec, 3),
prec_lag6 = lag(prec, 6),
t2_lag1 = lag(t2, 1),
t2_lag3 = lag(t2, 3),
prec_mois_lag1 = lag(prec_mois, 1),
month_sin = sin(2* pi* month / 12),
month_cos = cos(2* pi* month / 12),
snow_prec = ifelse(t2max < 0, prec, 0),
ppt_change = prec - lag(prec, 1),
rolling_prec3 = slider::slide_dbl(prec, mean, .before = 2, .complete = FALSE)
) |>
ungroup()
if (!is.null(clusters_df)) {
enriched <- left_join(enriched, clusters_df, by = "basin_id")
}
return(enriched)
}
# Enrich and log-transform baseflow
eval_245 <- enrich_eval_data(eval_ssp245) |> drop_na() |> mutate(log_baseflow = log(pmax(baseflow, 0.001)))
eval_585 <- enrich_eval_data(eval_ssp585) |> drop_na() |> mutate(log_baseflow = log(pmax(baseflow, 0.001)))
training_data <- enrich_eval_data(training_data) |> mutate(log_baseflow = log(pmax(baseflow, 0.001)))
gage_coords <- read_csv(here("1-data/site_lat-long.csv")) |>
mutate(basin_id = sprintf("%08d", as.integer(basin_id)))
# Column names
time_vars <- c("prec", "t2", "t2max", "t2min", "prec_mois",
"prec_lag1", "prec_lag2", "prec_lag3", "prec_lag6",
"t2_lag1", "t2_lag3", "prec_mois_lag1",
"month_sin", "month_cos", "snow_prec", "ppt_change", "rolling_prec3")
static_vars <- c("Area_km", "Elev_mean_m", "Elev_min_m", "Elev_max_m", "Relief_m", "lat", "lon")
timesteps <- 12
hyperparams <- tribble(
~cluster, ~units, ~dropout, ~learning_rate, ~batch_size,
1,        256,     0.25,      5e-4,            32,
2,        128,     0.25,      5e-4,            32,
3,        196,     0.25,      5e-4,            32,
4,        128,     0.25,      5e-4,            32
)
all_predictors <- c(time_vars, static_vars)
target_var <- "log_baseflow"
seq_len    <- timesteps
# Helper: ensemble predictions for one eval df (one cluster at a time inside)
ensemble_predict_one <- function(eval_df, model_dir) {
# Keep needed cols & drop NA
df <- eval_df %>%
dplyr::select(all_of(c(time_vars, static_vars, target_var,
"basin_id", "year", "month", "cluster"))) %>%
tidyr::drop_na(all_of(c(time_vars, static_vars, target_var)))
if (nrow(df) == 0) return(NULL)
# Build sequences
seq_eval <- make_sequences(df, seq_len, time_vars, static_vars, target = target_var)
if (nrow(seq_eval) == 0) return(NULL)
x_eval <- arrayify(seq_eval$X)
y_eval <- seq_eval$y
cl_id  <- unique(df$cluster)[1]
# Load seed perf + models
seed_perf <- readr::read_csv(file.path(model_dir, "seed_validation_metrics.csv"), show_col_types = FALSE)
seeds     <- sort(unique(seed_perf$seed))
paths <- file.path(model_dir, glue::glue("model_seed{seeds}.keras"))
names(paths) <- as.character(seeds)
models <- purrr::map(paths[file.exists(paths)], keras3::load_model)
if (length(models) == 0) return(NULL)
# Ensemble median prediction
if (length(models) == 1L) {
yhat <- as.numeric(models[[1]] |> predict(x_eval, verbose = 0))
} else {
yhat_mat <- sapply(models, function(m) as.numeric(m |> predict(x_eval, verbose = 0)))
yhat <- apply(as.matrix(yhat_mat), 1, median, na.rm = TRUE)
}
tibble::tibble(cluster = cl_id, y = y_eval, yhat = yhat)
}
predict_scenario <- function(eval_df, scenario_label) {
purrr::map_dfr(sort(unique(training_scaled$cluster)), function(cl) {
mdl_dir <- file.path(dir_models, glue::glue("cluster_{cl}"))
df_cl   <- eval_df %>% dplyr::filter(cluster == !!cl)
pr_cl   <- ensemble_predict_one(df_cl, mdl_dir)
if (is.null(pr_cl) || nrow(pr_cl) == 0) return(tibble::tibble())
pr_cl %>% dplyr::mutate(scenario = scenario_label)
}) %>%
dplyr::mutate(cluster = as.factor(cluster))
}
# Use it:
preds_245 <- predict_scenario(eval245_scaled, "SSP245")
# Fit scaler on training only, then apply to everything
scaler <- fit_scaler(training_data, all_predictors)
# ==== 3. Scaling helpers ====
fit_scaler <- function(df, cols) {
mu  <- sapply(df[cols], function(x) mean(x, na.rm = TRUE))
sig <- sapply(df[cols], function(x) sd(x,   na.rm = TRUE))
list(mu = mu, sig = sig)
}
apply_scaler <- function(df, scaler, cols) {
for (nm in cols) {
m  <- scaler$mu[[nm]]
sd <- scaler$sig[[nm]]
if (is.na(sd) || sd == 0) {  # guard against zero-variance cols
df[[nm]] <- 0
} else {
df[[nm]] <- (df[[nm]] - m) / sd
}
}
df
}
# Fit scaler on training only, then apply to everything
scaler <- fit_scaler(training_data, all_predictors)
training_scaled <- training_data %>% apply_scaler(scaler, all_predictors)
eval245_scaled  <- eval_245      %>% apply_scaler(scaler, all_predictors)
eval585_scaled  <- eval_585      %>% apply_scaler(scaler, all_predictors)
# Use it:
preds_245 <- predict_scenario(eval245_scaled, "SSP245")
preds_585 <- predict_scenario(eval585_scaled, "SSP585")
dir_models <- here("4-models/lstm")
# Use it:
preds_245 <- predict_scenario(eval245_scaled, "SSP245")
# ==== 4. Sequence builder ====
# Produces (X, y) with shape: [samples, seq_len, n_features]; we tile static vars across the time axis
make_sequences <- function(df, seq_len, time_vars, static_vars, target = "log_baseflow") {
df <- df %>% arrange(basin_id, year, month)
out <- map_dfr(split(df, df$basin_id), function(dbi) {
n <- nrow(dbi)
if (n <= seq_len) return(tibble())  # not enough rows for a sequence
X <- map(seq_len(n - seq_len), ~{
# Time-varying segment
Xt <- as.matrix(dbi[.x:(.x + seq_len - 1), time_vars, drop = FALSE])
# Static segment (repeat across timesteps)
Xs_row <- as.numeric(dbi[.x, static_vars, drop = FALSE])
Xs <- matrix(rep(Xs_row, each = seq_len), nrow = seq_len, byrow = FALSE)
cbind(Xt, Xs)
})
y <- dbi %>% slice((seq_len + 1):n) %>% pull(all_of(target))
tibble(
basin_id = dbi$basin_id[(seq_len + 1):n],
X = X,
y = y
)
})
out
}
arrayify <- function(X_list) {
if (length(X_list) == 0) return(NULL)
timesteps  <- nrow(X_list[[1]])
n_features <- ncol(X_list[[1]])
n_samples  <- length(X_list)
arr <- array(NA_real_, dim = c(n_samples, timesteps, n_features))
for (i in seq_along(X_list)) arr[i,,] <- X_list[[i]]
arr
}
# ==== 6. Model builder ====
build_lstm <- function(input_timesteps, input_features, units, dropout, learning_rate) {
input <- keras3::layer_input(shape = c(input_timesteps, input_features))
x <- input |>
keras3::layer_lstm(units = units, return_sequences = FALSE) |>
keras3::layer_dropout(rate = dropout) |>
keras3::layer_dense(units = 1)
model <- keras3::keras_model(inputs = input, outputs = x)
keras3::compile(
model,
optimizer = keras3::optimizer_adam(learning_rate = learning_rate),
loss = "mae",
metrics = list("mae")
)
return(model)
}
train_one_seed <- function(train_df, val_df, hp, time_vars, static_vars, target, save_dir) {
set.seed(hp$.seed); tf$random$set_seed(hp$.seed)
# Build sequences
tr_seq <- make_sequences(train_df, hp$seq_len, time_vars, static_vars, target)
va_seq <- make_sequences(val_df,   hp$seq_len, time_vars, static_vars, target)
x_tr <- arrayify(tr_seq$X); y_tr <- tr_seq$y
x_va <- arrayify(va_seq$X); y_va <- va_seq$y
stopifnot(!is.null(x_tr), !is.null(x_va))
# Build model
model <- build_lstm(
input_timesteps = dim(x_tr)[2],
input_features  = dim(x_tr)[3],
units           = hp$units,
dropout         = hp$dropout,
learning_rate   = hp$learning_rate
)
callbacks <- list(
callback_early_stopping(monitor = "val_loss", patience = 5, restore_best_weights = TRUE)
)
# Fit model
hist <- model |>
fit(
x = x_tr, y = y_tr,
validation_data = list(x_va, y_va),
epochs = hp$epochs,
batch_size = hp$batch_size,
verbose = 0,
callbacks = callbacks
)
# Validation performance
pred_va <- as.numeric(model |> predict(x_va, verbose = 0))
perf <- tibble(
seed = hp$.seed,
mae  = mae_vec(y_va, pred_va),
rmse = rmse_vec(y_va, pred_va),
nse  = nse_vec(y_va, pred_va)
)
# Save full model (.keras)
dir.create(save_dir, recursive = TRUE, showWarnings = FALSE)
mpath <- file.path(save_dir, glue("model_seed{hp$.seed}.keras"))
keras3::save_model(model, mpath)
list(perf = perf, model_path = mpath)
}
train_seed_ensemble <- function(train_df, val_df, hp, time_vars, static_vars, target, save_dir, seeds) {
dir.create(save_dir, recursive = TRUE, showWarnings = FALSE)
# Train one model per seed
res <- map(seeds, function(s) {
hp2 <- hp; hp2$.seed <- s
train_one_seed(train_df, val_df, hp2, time_vars, static_vars, target, save_dir)
})
# Collect performances
perf <- bind_rows(lapply(res, `[[`, "perf"))
# Pick best seed by validation MAE
best_seed <- perf %>% arrange(mae) %>% slice(1) %>% pull(seed)
# Save performance summary
readr::write_csv(perf, file.path(save_dir, "seed_validation_metrics.csv"))
readr::write_lines(as.character(best_seed), file.path(save_dir, "best_seed.txt"))
list(
perf = perf,
best_seed = best_seed,
model_paths = map_chr(res, "model_path"),
seeds = sort(unique(perf$seed))
)
}
training_clean <- training_scaled %>%
drop_na(all_of(time_vars))
# Use it:
preds_245 <- predict_scenario(eval245_scaled, "SSP245")
preds_585 <- predict_scenario(eval585_scaled, "SSP585")
preds_all <- dplyr::bind_rows(preds_245, preds_585)
# Residuals (pred - obs)
res_df <- preds_all %>%
mutate(
cluster = factor(cluster, levels = c("1","2","3","4")),
resid   = yhat - y
)
# Palette (same as before)
clrs <- c(`1`="#7db030", `2`="#ff3432", `3`="#FFB81F", `4`="#2487EE")
# --- Plot 1: Residual distributions by cluster (violin + box), faceted by scenario
p_res_violin <- ggplot(res_df, aes(x = cluster, y = resid, fill = cluster)) +
geom_violin(alpha = 0.25, color = NA, scale = "width") +
geom_boxplot(width = 0.18, outlier.size = 0.6, fatten = 1, alpha = 0.8) +
geom_hline(yintercept = 0, linetype = "dashed", color = "grey40") +
facet_grid(rows = vars(scenario)) +
scale_fill_manual(values = clrs, guide = "none") +
labs(x = "Cluster", y = "Residual (pred - obs)") +
theme_minimal(base_size = 12) +
theme(
panel.grid.minor = element_blank(),
panel.border     = element_rect(color = "black", fill = NA, linewidth = 0.5),
strip.background = element_rect(fill = "grey85", color = "black"),
axis.title       = element_text(face = "bold"),
axis.text        = element_text(color = "grey20")
)
# --- Plot 2: Residual density by cluster (colored), faceted by scenario
p_res_dens <- ggplot(res_df, aes(x = resid, color = cluster)) +
geom_density(linewidth = 0.9, alpha = 0.9) +
geom_vline(xintercept = 0, linetype = "dashed", color = "grey40") +
facet_grid(rows = vars(scenario)) +
scale_color_manual(values = clrs, name = "Cluster") +
labs(x = "Residual (pred - obs)", y = "Density") +
theme_minimal(base_size = 12) +
theme(
panel.grid.minor = element_blank(),
panel.border     = element_rect(color = "black", fill = NA, linewidth = 0.5),
strip.background = element_rect(fill = "grey85", color = "black"),
axis.title       = element_text(face = "bold"),
axis.text        = element_text(color = "grey20")
)
# Print
p_res_violin
p_res_dens
# Save
ggsave(file.path(dir_models, "fig_residuals_violin_by_cluster.png"),
p_res_violin, width = 7.5, height = 6.0, dpi = 600)
ggsave(file.path(dir_models, "fig_residuals_density_by_cluster.png"),
p_res_dens, width = 7.5, height = 6.0, dpi = 600)
readr::write_csv(res_summary, file.path(dir_models, "residual_summary_by_cluster_scenario.csv"))
getwd()
